
import os
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, pipeline
from vncorenlp import VnCoreNLP
from dateutil.parser import parse
from datetime import datetime, timedelta
import logging
import re
import os
import calendar
import torch
from extract_location import extract_location
from extract_time import extract_time
from extract_price import extract_price_vn
from pydantic import BaseModel
from google_genAI import get_genai_response
from retrieval import RetrievalPipeline


# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)
current_dir = os.path.dirname(os.path.abspath(__file__))


class ResetRequest(BaseModel):
    user_id: str = "default_user"

class SessionManager:
    """Qu·∫£n l√Ω session ng∆∞·ªùi d√πng v·ªõi TTL."""
    def __init__(self, ttl_hours=24):
        self.sessions = {}
        self.ttl = timedelta(hours=ttl_hours)

    def get_session(self, user_id):
        if user_id not in self.sessions or self._is_expired(user_id):
            self.sessions[user_id] = {
                "location": None, "time": None, "price": None,
                "last_updated": datetime.now(), "search_history": []
            }
        return self.sessions[user_id]

    def _is_expired(self, user_id):
        session = self.sessions.get(user_id, {})
        last_updated = session.get("last_updated")
        return last_updated and (datetime.now() - last_updated) > self.ttl

    def reset_session(self, user_id):
        self.sessions[user_id] = {
            "location": None, "time": None, "price": None,
            "last_updated": datetime.now(), "search_history": []
        }
        logger.debug(f"Reset session for user_id: {user_id}")

class TourRetrievalPipeline:
    """Pipeline ƒë·ªÉ t√¨m ki·∫øm v√† tr·∫£ l·ªùi th√¥ng tin tour du l·ªãch."""
    INTENT_LABELS = {
        0: "find_tour_with_location",
        1: "find_tour_with_time",
        2: "find_tour_with_price",
        3: "find_tour_with_location_and_time",
        4: "find_tour_with_location_and_price",
        5: "find_tour_with_time_and_price",
        6: "find_with_all",
        7: "out_of_scope"
     }

    def __init__(self, index_file="faq_index.faiss", metadata_file="faq_metadata.json"):
        # Load Faiss index v√† metadata
        self.index = faiss.read_index(index_file)
        with open(metadata_file, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)

        # Load SentenceTransformer
        self.model = SentenceTransformer("all-MiniLM-L6-v2")
        self.retrievalPipeline = RetrievalPipeline()

        # Load PhoBERT cho ph√¢n lo·∫°i √Ω ƒë·ªãnh
        model_intent_path = os.path.join(current_dir, "phobert_intent_finetuned")
        self.intent_tokenizer = AutoTokenizer.from_pretrained(model_intent_path)
        self.intent_model = AutoModelForSequenceClassification.from_pretrained(model_intent_path)
        # self.intent_labels = {0: "find_tour_with_location", 1: "find_tour_with_location_and_time", 2: "find_tour_with_location_and_price", 3: "out_of_scope"}


        jar_path = os.path.join(current_dir, "VnCoreNLP-1.1.1.jar")
        try:
            self.vncorenlp = VnCoreNLP(jar_path, annotators="wseg,pos,ner", max_heap_size='-Xmx2g')
        except Exception as e:
            logger.error(f"Kh√¥ng th·ªÉ kh·ªüi t·∫°o VnCoreNLP: {e}")
            raise

        self.session_manager = SessionManager()
        logger.info("Kh·ªüi t·∫°o TourRetrievalPipeline th√†nh c√¥ng!")

    def extract_intent(self, query):
        try:
            inputs = self.intent_tokenizer(query, return_tensors="pt", max_length=128, truncation=True, padding=True)
            with torch.no_grad():
                outputs = self.intent_model(**inputs)
            predicted_class = torch.argmax(outputs.logits, dim=1).item()
            return self.INTENT_LABELS[predicted_class]
        except Exception as e:
            logger.error(f"L·ªói ph√¢n lo·∫°i √Ω ƒë·ªãnh: {e}")
            return "out_of_scope"

    def extract_entities(self, query,intent, user_id="default_user"):
        session = self.session_manager.get_session(user_id)
        location = session["location"]
        time = session["time"]
        price = session["price"]


        if intent in ["find_tour_with_location", "find_tour_with_location_and_time", "find_tour_with_location_and_price", "find_with_all"]:
            extracted_location = extract_location(query)
            if extracted_location != "None":
                location = extracted_location
                session["location"] = location
                logger.debug(f"Extracted location: {location}")

        if intent in  ["find_tour_with_location_and_time", "find_tour_with_time", "find_tour_with_time_and_price", "find_with_all"]:
            extracted_time = extract_time(query)
            if extracted_time != "None":
                time = extracted_time
                session["time"] = time
                logger.debug(f"Extracted time: {time}")

        if intent in ["find_tour_with_location_and_price", "find_tour_with_price", "find_tour_with_time_and_price", "find_with_all"]:
            extracted_price = extract_price_vn(query)
            if extracted_price != "None":
                price = extracted_price
                session["price"] = price
                logger.debug(f"Extracted price: {price}")
       
        session["last_updated"] = datetime.now()
        session["search_history"].append({"query": query})
        logger.debug(f"Extracted entities: location={location}, time={time}, price={price}")
        return {"location": location, "time": time, "price": price}

    def reset_session(self, user_id):
            self.session_manager.reset_session(user_id)
            logger.debug(f"Session reset for user_id: {user_id}")

    def get_faq_response(self, query, k=1):
        try:
            respond = self.retrievalPipeline.get_retrieved_context(query, top_k=k)
            response_text = get_genai_response(
                "Ki·ªÉm tra xem c√¢u query n√†y: '" + query + "' v√† c√¢u tr·∫£ l·ªùi n√†y: '" + respond + 
                "' c√≥ ph√π h·ª£p kh√¥ng, n·∫øu ph√π h·ª£p th√¨ tr·∫£ l·ªùi y nh∆∞ c√¢u tr·∫£ l·ªùi ƒë√≥, n·∫øu kh√¥ng th√¨ ch·ªâ c·∫ßn tr·∫£ l·ªùi l√† kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y, v√† h·ªèi xem kh√°ch h√†ng c√≥ mu·ªën h·ªèi v·ªÅ tour kh√¥ng? "
            )
            return {
                "response": response_text,
            }
        except Exception as e:
            logger.error(f"L·ªói t√¨m FAQ v·ªõi KNN: {e}")
            return {
                "response": "D·∫°, em ch∆∞a hi·ªÉu √Ω b·∫°n. B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ tour ho·∫∑c ƒë·∫∑t ph√≤ng kh√°ch s·∫°n kh√¥ng ·∫°?",
            }

    def get_tour_response(self, query, user_id="default_user"):
        intent = self.extract_intent(query)
        print("intent: ", intent)
        if intent == "out_of_scope":
            faq_response = self.get_faq_response(query)
            return {
                    "status": "faq",
                    "response": f"{faq_response['response']}",
                    "location": "None",
                    "time": "None",
                    "price": "None"
                }
        
        info = self.extract_entities(query, intent, user_id)

        missing_info = []
        if not info["location"]:
            missing_info.append("ƒêi·ªÉm ƒë·∫øn (v√≠ d·ª•: ƒê√† L·∫°t, H√† N·ªôi, Ph√∫ Qu·ªëc,...)")
        if not info["time"]:
            missing_info.append("Th·ªùi gian kh·ªüi h√†nh (v√≠ d·ª•: th√°ng 12, 2 tu·∫ßn t·ªõi,...)")
        if not info["price"]:
            missing_info.append("Gi√° (v√≠ d·ª•: d∆∞·ªõi 5 tri·ªáu, kho·∫£ng 2 tri·ªáu,...)")

        if missing_info:
            prompt = (
                f"Ng∆∞·ªùi d√πng ƒë√£ cung c·∫•p th√¥ng tin v·ªÅ tour du l·ªãch: "
                f"ƒê·ªãa ƒëi·ªÉm: {info['location'] or 'ch∆∞a c√≥'}, "
                f"Th·ªùi gian: {info['time'] or 'ch∆∞a c√≥'}, "
                f"Gi√°: {info['price'] or 'ch∆∞a c√≥'}. "
                f"Vui l√≤ng vi·∫øt m·ªôt c√¢u tr·∫£ l·ªùi l·ªãch s·ª± b·∫±ng ti·∫øng Vi·ªát, x√°c nh·∫≠n th√¥ng tin ƒë√£ nh·∫≠n v√† y√™u c·∫ßu cung c·∫•p c√°c th√¥ng tin c√≤n thi·∫øu (th√¥ng tin s·∫Ω cung c·∫•p sau kh√¥ng c·∫ßn n√≥i ra c·ª• th·ªÉ) "
                f"N√≥i ki·ªÉu c√¢u cu·ªëi c√πng c√≥ d·∫•u :"
                f"V√≠ d·ª•: 'D·∫°, em ƒë√£ ghi nh·∫≠n th√¥ng tin qu√Ω kh√°ch mu·ªën t√¨m tour ƒë·∫øn [ƒë·ªãa ƒëi·ªÉm]/[th·ªùi gian kh·ªüi h√†nh]/[gi√° tour]. Xin qu√Ω kh√°ch vui l√≤ng cho em bi·∫øt th√™m th√¥ng tin nh∆∞:'"
            )
            try:
                response_text = get_genai_response(prompt)
                return {
                    "status": "missing_info",
                    "response": f"{response_text}\n- " + "\n- ".join(missing_info),
                    "location": info['location'] or "None",
                    "time": info['time'] or "None",
                    "price": info['price'] or "None"
                }
            except Exception as e:
                logger.error(f"L·ªói g·ªçi Gemini API: {e}")
                return {
                    "status": "missing_info",
                    "response": "D·∫°, ƒë·ªÉ t√¨m tour ph√π h·ª£p, em c·∫ßn b·∫°n cung c·∫•p th√™m:\n- " + "\n- ".join(missing_info),
                    "location": info['location'] or "None",
                    "time": info['time'] or "None",
                    "price": info['price'] or "None"
                }
        # ƒê·ªß th√¥ng tin th√¨ t√¨m ki·∫øm
        prompt = (
                f"ƒê√£ c√≥ c√°c th√¥ng tin v·ªÅ tour du l·ªãch: "
                f"ƒê·ªãa ƒëi·ªÉm: {info['location']}, "
                f"Th·ªùi gian: {info['time']}, "
                f"Gi√°: {info['price']}. "
                f"Vui l√≤ng vi·∫øt m·ªôt c√¢u tr·∫£ l·ªùi l·ªãch s·ª± b·∫±ng ti·∫øng Vi·ªát, x√°c nh·∫≠n th√¥ng tin ƒë√£ nh·∫≠n v√† ƒë∆∞a ra c√¢u m·ªü ƒë·∫ßu tr∆∞·ªõc khi li·ªát k√™ c√°c tour ƒë√£ t√¨m ƒë∆∞·ª£c. "
                f"N√≥i ki·ªÉu c√¢u cu·ªëi c√πng c√≥ d·∫•u :"
                f"V√≠ d·ª•: 'D·∫°, em ƒë√£ t√¨m ƒë∆∞·ª£c m·ªôt s·ªë tour ph√π h·ª£p nh∆∞: "
            )
        response_text = get_genai_response(prompt)
        response_obj = {
            "status": "success",
            "response": f"{response_text}",
            "location": info['location'] or "None",
            "time": info['time'] or "None",
            "price": info['price'] or "None"
        }
        self.reset_session("default_user")
        return response_obj




# if __name__ == "__main__":
#     current_dir = os.path.dirname(os.path.abspath(__file__))
#     pipeline = TourRetrievalPipeline(
#         index_file=os.path.join(current_dir, "faq_index.faiss"),
#         metadata_file=os.path.join(current_dir, "faq_metadata.json")
#     )
#     user_id = "test_user"
#     while True:
#         user_query = input("B·∫°n: ")
#         if user_query.lower() in ["exit", "quit"]:
#             break
#         response = pipeline.get_tour_response(user_query, user_id=user_id)
#         print(f"Bot: {response}")









# import os
# import json
# import faiss
# import numpy as np
# from sentence_transformers import SentenceTransformer
# from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, pipeline
# from subprocess import Popen, PIPE
# from dateutil.parser import parse
# from datetime import datetime, timedelta
# import logging
# import re
# import calendar
# import torch
# import tempfile

# # C·∫•u h√¨nh logging
# logging.basicConfig(level=logging.DEBUG)
# logger = logging.getLogger(__name__)

# class TourRetrievalPipeline:
#     def __init__(self, index_file="faq_index.faiss", metadata_file="faq_metadata.json"):
#         # Load Faiss index v√† metadata
#         self.index = faiss.read_index(index_file)
#         with open(metadata_file, 'r', encoding='utf-8') as f:
#             self.metadata = json.load(f)

#         # Load SentenceTransformer
#         self.model = SentenceTransformer("all-MiniLM-L6-v2")

#         # Load PhoBERT cho ph√¢n lo·∫°i √Ω ƒë·ªãnh
#         self.intent_tokenizer = AutoTokenizer.from_pretrained("./phobert_intent_finetuned")
#         self.intent_model = AutoModelForSequenceClassification.from_pretrained("./phobert_intent_finetuned")
#         self.intent_labels = {0: "find_tour", 1: "book_room_with_num_people", 2: "book_room_with_number"}

#         # Load PhoBERT cho NER
#         self.ner_tokenizer = AutoTokenizer.from_pretrained("./phobert_ner_finetuned")
#         self.ner_model = AutoModelForTokenClassification.from_pretrained("./phobert_ner_finetuned")
#         self.ner_pipeline = pipeline("ner", model=self.ner_model, tokenizer=self.ner_tokenizer, aggregation_strategy="simple")
#         self.label_list = ["O", "B-LOC", "B-DATE", "I-DATE", "B-num_people", "B-num_rooms"]
#         self.id2label = {i: label for i, label in enumerate(self.label_list)}

#        # Kh·ªüi t·∫°o VnCoreNLP nh∆∞ ti·∫øn tr√¨nh con
#         current_dir = os.path.dirname(os.path.abspath(__file__))
#         self.jar_path = os.path.join(current_dir, "VnCoreNLP-1.2.jar")
#         models_path = os.path.join(current_dir, "models", "wordsegmenter", "wordsegmenter.rdr")
#         if not os.path.exists(self.jar_path):
#             raise FileNotFoundError(f"File {self.jar_path} kh√¥ng t·ªìn t·∫°i!")
#         if not os.path.exists(models_path):
#             raise FileNotFoundError(f"File m√¥ h√¨nh {models_path} kh√¥ng t·ªìn t·∫°i! Vui l√≤ng ƒë·∫£m b·∫£o th∆∞ m·ª•c 'models' n·∫±m c√πng c·∫•p v·ªõi file .jar.")
#         logger.info(f"ƒêang chu·∫©n b·ªã s·ª≠ d·ª•ng VnCoreNLP v·ªõi file: {self.jar_path}")

#         # Qu·∫£n l√Ω session
#         self.sessions = {}
#         logger.info("Kh·ªüi t·∫°o TourRetrievalPipeline th√†nh c√¥ng!")

#     def tokenize_with_vncorenlp(self, query):
#         print("b·∫Øt ƒë·∫ßu tokenize")
#         # T·∫°o file t·∫°m ƒë·ªÉ l∆∞u query
#         with tempfile.NamedTemporaryFile(mode='w',encoding='utf-8', suffix='.txt', delete=False) as input_file:
#             input_file.write(query)
#             input_file_path = input_file.name
        
#         # T·∫°o file t·∫°m cho ƒë·∫ßu ra
#         output_file_path = input_file_path + '.out'

#         # Ch·∫°y VnCoreNLP nh∆∞ ti·∫øn tr√¨nh con
#         command = [
#             "java", "-Xmx2g", "-jar", self.jar_path,
#             "-fin", input_file_path,
#             "-fout", output_file_path,
#             "-annotators", "wseg,pos,ner"
#         ]
#         process = Popen(command, stdout=PIPE, stderr=PIPE)
#         stdout, stderr = process.communicate()
#         if process.returncode != 0:
#             logger.error(f"L·ªói khi ch·∫°y VnCoreNLP: {stderr.decode()}")
#             raise Exception(f"L·ªói khi ch·∫°y VnCoreNLP: {stderr.decode()}")

#         # ƒê·ªçc k·∫øt qu·∫£
#         with open(output_file_path, 'r', encoding='utf-8') as f:
#             result = f.read()
#         logger.debug(f"K·∫øt qu·∫£ tokenize t·ª´ VnCoreNLP: {result}")

#         # X√≥a file t·∫°m
#         os.unlink(input_file_path)
#         os.unlink(output_file_path)

#         return result.splitlines()  # Tr·∫£ v·ªÅ danh s√°ch c√°c d√≤ng token

#     def extract_intent(self, query):
#         inputs = self.intent_tokenizer(query, return_tensors="pt", max_length=128, truncation=True, padding=True)
#         with torch.no_grad():
#             outputs = self.intent_model(**inputs)
#         logits = outputs.logits
#         predicted_class = torch.argmax(logits, dim=1).item()
#         print("predicted_class: ",predicted_class)
#         intent = self.intent_labels[predicted_class]
#         logger.debug(f"Detected intent: {intent}")
#         return intent

#     def extract_entities(self, query, user_id="default_user"):
#         if user_id not in self.sessions:
#             self.sessions[user_id] = {
#                 "location": None, "time": None, "time_display": None,
#                 "departure": None, "num_people": None, "num_rooms": None
#             }

#         session = self.sessions[user_id]

#         # S·ª≠a l·ªói ch√≠nh t·∫£
#         query = query.replace("tron", "trong")
#         logger.debug(f"Query sau khi s·ª≠a l·ªói ch√≠nh t·∫£: {query}")

#         # Tokenize b·∫±ng VnCoreNLP
#         tokens = self.tokenize_with_vncorenlp(query)
#         tokens = [word for sentence in tokens for word in sentence]  # Gi·∫£ ƒë·ªãnh m·ªói d√≤ng l√† m·ªôt c√¢u ƒë√£ ph√¢n ƒëo·∫°n
#         logger.debug(f"Tokens: {tokens}")

#         # Tr√≠ch xu·∫•t th·ª±c th·ªÉ b·∫±ng PhoBERT NER
#         entities = self.ner_pipeline(query)
#         logger.debug(f"Entities: {entities}")

#         location = session["location"]
#         time = session["time"]
#         time_display = session["time_display"]
#         departure = session["departure"]
#         num_people = session["num_people"]
#         num_rooms = session["num_rooms"]

#         # Chuy·ªÉn ƒë·ªïi t·ª´ ti·∫øng Vi·ªát sang s·ªë (n·∫øu c·∫ßn)
#         word_to_num = {"m·ªôt": 1, "hai": 2, "ba": 3, "b·ªën": 4, "nƒÉm": 5, "s√°u": 6, "b·∫£y": 7, "t√°m": 8, "ch√≠n": 9, "m∆∞·ªùi": 10}

#         # Tr√≠ch xu·∫•t t·ª´ entities
#         for entity in entities:
#             entity_label = entity["entity_group"]
#             entity_text = entity["word"]
#             if entity_label == "B-LOC":
#                 if not location:
#                     location = entity_text
#                 else:
#                     departure = entity_text
#             elif entity_label == "B-DATE":
#                 try:
#                     time = parse(entity_text, fuzzy=True)
#                     time_display = entity_text
#                     logger.debug(f"Parsed time as datetime: {time}")
#                 except ValueError as e:
#                     logger.warning(f"Kh√¥ng parse ƒë∆∞·ª£c {entity_text} th√†nh datetime: {e}")
#                     time_display = entity_text
#             elif entity_label == "B-num_people":
#                 num_people = word_to_num.get(entity_text.lower(), int(entity_text))
#             elif entity_label == "B-num_rooms":
#                 num_rooms = word_to_num.get(entity_text.lower(), int(entity_text))

#         # Logic th·ªß c√¥ng b·ªï sung cho th·ªùi gian
#         query_lower = query.lower()
#         month_names = {
#             "th√°ng 1": 1, "th√°ng 2": 2, "th√°ng 3": 3, "th√°ng 4": 4, "th√°ng 5": 5,
#             "th√°ng 6": 6, "th√°ng 7": 7, "th√°ng 8": 8, "th√°ng 9": 9, "th√°ng 10": 10,
#             "th√°ng 11": 11, "th√°ng 12": 12
#         }
#         for month_name, month_num in month_names.items():
#             if month_name in query_lower and not time:
#                 current_year = datetime.now().year
#                 if "n√†y" in query_lower and month_num < datetime.now().month:
#                     current_year += 1
#                 start_date = datetime(current_year, month_num, 1)
#                 end_date = datetime(current_year, month_num, calendar.monthrange(current_year, month_num)[1])
#                 time = (start_date, end_date)
#                 time_display = month_name
#                 break

#         date_pattern = r'\d{1,2}/\d{1,2}(?:/\d{2,4})?'
#         match = re.search(date_pattern, query)
#         if match and not time:
#             date_str = match.group(0)
#             try:
#                 if len(date_str.split('/')) == 2:
#                     date_str = f"{date_str}/{datetime.now().year}"
#                 parsed_date = parse(date_str, dayfirst=True)
#                 time = (parsed_date, parsed_date)
#                 time_display = date_str
#                 logger.debug(f"Parsed custom date {date_str} as: {parsed_date}")
#             except ValueError as e:
#                 logger.warning(f"Kh√¥ng parse ƒë∆∞·ª£c {date_str} th√†nh datetime: {e}")
#                 time_display = date_str

#         # C·∫≠p nh·∫≠t session
#         if location:
#             session["location"] = location
#         if time:
#             session["time"] = time
#             session["time_display"] = time_display
#         if departure:
#             session["departure"] = departure
#         if num_people:
#             session["num_people"] = num_people
#         if num_rooms:
#             session["num_rooms"] = num_rooms

#         logger.debug(f"Extracted entities: location={location}, time={time}, time_display={time_display}, departure={departure}, num_people={num_people}, num_rooms={num_rooms}")
#         return {
#             "location": location, "time": time, "time_display": time_display,
#             "departure": departure, "num_people": num_people, "num_rooms": num_rooms
#         }

#     def get_tour_response(self, query, top_k=3, user_id="default_user"):
#         intent = self.extract_intent(query)
#         info = self.extract_entities(query, user_id)
#         session = self.sessions[user_id]

#         if intent == "find_tour":
#             if not info["location"]:
#                 return "D·∫°, ƒë·ªÉ t√¨m tour ph√π h·ª£p, em c·∫ßn b·∫°n cung c·∫•p th√™m:\n- ƒêi·ªÉm ƒë·∫øn (v√≠ d·ª•: ƒê√† L·∫°t, H√† N·ªôi, Ph√∫ Qu·ªëc)\nB·∫°n cho em bi·∫øt ƒëi·ªÉm ƒë·∫øn nha!"
#             if not info["time"]:
#                 return "D·∫°, em c·∫ßn b·∫°n cung c·∫•p th√™m:\n- Th·ªùi gian kh·ªüi h√†nh (v√≠ d·ª•: th√°ng 12, 2 tu·∫ßn t·ªõi)\nB·∫°n cho em bi·∫øt th·ªùi gian nha!"
#             if not info["departure"]:
#                 return "D·∫°, em c·∫ßn b·∫°n cung c·∫•p th√™m:\n- ƒê·ªãa ƒëi·ªÉm kh·ªüi h√†nh (v√≠ d·ª•: t·ª´ TP.HCM, H√† N·ªôi)\nB·∫°n cho em bi·∫øt ƒëi·ªÉm kh·ªüi h√†nh nha!"

#             query_description = f"Tour t·∫°i {info['location']}"
#             query_embedding = self.model.encode([query_description], show_progress_bar=False)
#             query_embedding = np.array(query_embedding).astype("float32")

#             distances, indices = self.index.search(query_embedding, top_k)
#             logger.debug(f"Faiss search results - distances: {distances}, indices: {indices}")

#             start_date = info["time"][0] if isinstance(info["time"], tuple) else info["time"]
#             end_date = info["time"][1] if isinstance(info["time"], tuple) else info["time"]
#             if isinstance(end_date, str):
#                 try:
#                     end_date = parse(end_date, fuzzy=True) + timedelta(days=14)
#                 except ValueError:
#                     end_date = start_date + timedelta(days=14)

#             filtered_tours = []
#             for idx in indices[0]:
#                 if idx < len(self.metadata):
#                     tour = self.metadata[idx]
#                     if tour["destination"].lower() != info["location"].lower():
#                         continue
#                     tour_dates = [datetime.strptime(d, "%Y-%m-%d") for d in tour["start_dates"]]
#                     if not any(start_date <= d <= end_date for d in tour_dates):
#                         continue
#                     if info["departure"] and info["departure"].lower() not in [d.lower() for d in tour["departure"]]:
#                         continue
#                     filtered_tours.append(tour)

#             if not filtered_tours:
#                 response = f"Xin l·ªói, em ch∆∞a t√¨m th·∫•y tour n√†o ƒë·∫øn {info['location']} trong th·ªùi gian {info['time_display']}"
#                 if info["departure"]:
#                     response += f" t·ª´ {info['departure']}"
#                 response += ". B·∫°n mu·ªën th·ª≠ ƒë·ªãa ƒëi·ªÉm kh√°c ho·∫∑c th·ªùi gian kh√°c kh√¥ng n√®?"
#                 return response

#             response = f"D·∫°, b·∫°n mu·ªën kh√°m ph√° {info['location']} trong {info['time_display']}"
#             if info["departure"]:
#                 response += f" t·ª´ {info['departure']}"
#             response += "! üòä D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë tour g·ª£i √Ω:\n"
#             for tour in filtered_tours:
#                 response += f"\n- **{tour['name']}**\n"
#                 response += f"  - S·ªë ng√†y: {tour['duration']}\n"
#                 response += f"  - Ng√†y kh·ªüi h√†nh: {', '.join(tour['start_dates'])}\n"
#                 response += f"  - Gi√° t·ª´: {tour['price']:,}‚Ç´\n"
#                 response += f"  - ƒêi·ªÉm n·ªïi b·∫≠t: {', '.join(tour['highlights'])}.\n"
#                 response += f"  - Chi ti·∫øt: {tour['details_url']}\n"
#             response += "\nB·∫°n ∆∞ng tour n√†o kh√¥ng? üòä Mu·ªën em g·ª≠i chi ti·∫øt l·ªãch tr√¨nh hay h·ªó tr·ª£ ƒë·∫∑t tour lu√¥n n√®?"

#             # X√≥a session sau khi ho√†n th√†nh
#             self.sessions.pop(user_id, None)
#             return response

#         elif intent == "book_room_with_num_people":
#             if not info["location"]:
#                 return "D·∫°, ƒë·ªÉ ƒë·∫∑t ph√≤ng, em c·∫ßn b·∫°n cung c·∫•p th√™m:\n- ƒêi·ªÉm ƒë·∫øn (v√≠ d·ª•: ƒê√† L·∫°t, H√† N·ªôi, Ph√∫ Qu·ªëc)\nB·∫°n cho em bi·∫øt ƒëi·ªÉm ƒë·∫øn nha!"
#             if not info["time"]:
#                 return "D·∫°, em c·∫ßn b·∫°n cung c·∫•p th√™m:\n- Th·ªùi gian l∆∞u tr√∫ (v√≠ d·ª•: th√°ng 12, 2 tu·∫ßn t·ªõi)\nB·∫°n cho em bi·∫øt th·ªùi gian nha!"
#             if not info["num_people"]:
#                 return "D·∫°, em c·∫ßn bi·∫øt s·ªë l∆∞·ª£ng ng∆∞·ªùi ƒë·ªÉ ƒë·∫∑t ph√≤ng. B·∫°n cho em bi·∫øt s·ªë ng∆∞·ªùi nha!"

#             response = f"D·∫°, b·∫°n mu·ªën ƒë·∫∑t ph√≤ng t·∫°i {info['location']} cho {info['num_people']} ng∆∞·ªùi trong {info['time_display']}."
#             if info["departure"]:
#                 response += f" ƒêi·ªÉm kh·ªüi h√†nh t·ª´ {info['departure']}."
#             response += "\nEm s·∫Ω t√¨m kh√°ch s·∫°n ph√π h·ª£p v√† g·ª£i √Ω tour ƒëi k√®m n·∫øu c·∫ßn. B·∫°n ch·ªù em m·ªôt ch√∫t nha! üòä"

#             # X·ª≠ l√Ω gi·∫£ l·∫≠p t√¨m kh√°ch s·∫°n
#             response += "\nG·ª£i √Ω kh√°ch s·∫°n:\n- **Kh√°ch s·∫°n Thanh L·ªãch** (Hu·∫ø)\n  - Ph√≤ng ƒë√¥i: 500,000‚Ç´/ƒë√™m\n  - Ph√≤ng gia ƒë√¨nh: 800,000‚Ç´/ƒë√™m\n  - ƒê·∫∑t ngay: [Link ƒë·∫∑t ph√≤ng]\n"
#             response += "\nB·∫°n c√≥ mu·ªën em t√¨m th√™m tour ƒëi k√®m kh√¥ng n√®?"

#             self.sessions.pop(user_id, None)
#             return response

#         elif intent == "book_room_with_number":
#             if not info["location"]:
#                 return "D·∫°, ƒë·ªÉ ƒë·∫∑t ph√≤ng, em c·∫ßn b·∫°n cung c·∫•p th√™m:\n- ƒêi·ªÉm ƒë·∫øn (v√≠ d·ª•: ƒê√† L·∫°t, H√† N·ªôi, Ph√∫ Qu·ªëc)\nB·∫°n cho em bi·∫øt ƒëi·ªÉm ƒë·∫øn nha!"
#             if not info["time"]:
#                 return "D·∫°, em c·∫ßn b·∫°n cung c·∫•p th√™m:\n- Th·ªùi gian l∆∞u tr√∫ (v√≠ d·ª•: th√°ng 12, 2 tu·∫ßn t·ªõi)\nB·∫°n cho em bi·∫øt th·ªùi gian nha!"
#             if not info["num_rooms"]:
#                 return "D·∫°, em c·∫ßn bi·∫øt s·ªë l∆∞·ª£ng ph√≤ng ƒë·ªÉ ƒë·∫∑t. B·∫°n cho em bi·∫øt s·ªë ph√≤ng nha!"

#             response = f"D·∫°, b·∫°n mu·ªën ƒë·∫∑t {info['num_rooms']} ph√≤ng t·∫°i {info['location']} trong {info['time_display']}."
#             if info["departure"]:
#                 response += f" ƒêi·ªÉm kh·ªüi h√†nh t·ª´ {info['departure']}."
#             response += "\nEm s·∫Ω t√¨m kh√°ch s·∫°n ph√π h·ª£p v√† g·ª£i √Ω tour ƒëi k√®m n·∫øu c·∫ßn. B·∫°n ch·ªù em m·ªôt ch√∫t nha! üòä"

#             # X·ª≠ l√Ω gi·∫£ l·∫≠p t√¨m kh√°ch s·∫°n
#             response += "\nG·ª£i √Ω kh√°ch s·∫°n:\n- **Kh√°ch s·∫°n Thanh L·ªãch** (Hu·∫ø)\n  - Ph√≤ng ƒë√¥i: 500,000‚Ç´/ƒë√™m\n  - Ph√≤ng gia ƒë√¨nh: 800,000‚Ç´/ƒë√™m\n  - ƒê·∫∑t ngay: [Link ƒë·∫∑t ph√≤ng]\n"
#             response += "\nB·∫°n c√≥ mu·ªën em t√¨m th√™m tour ƒëi k√®m kh√¥ng n√®?"

#             self.sessions.pop(user_id, None)
#             return response

#         else:
#             return "D·∫°, em ch∆∞a hi·ªÉu √Ω b·∫°n. B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ tour ho·∫∑c ƒë·∫∑t ph√≤ng kh√°ch s·∫°n kh√¥ng ·∫°?"

# if __name__ == "__main__":
#     current_dir = os.path.dirname(os.path.abspath(__file__))
#     pipeline = TourRetrievalPipeline(
#         index_file=os.path.join(current_dir, "faq_index.faiss"),
#         metadata_file=os.path.join(current_dir, "faq_metadata.json")
#     )
#     user_id = "test_user"
#     while True:
#         user_query = input("B·∫°n: ")
#         if user_query.lower() in ["exit", "quit"]:
#             break
#         response = pipeline.get_tour_response(user_query, user_id=user_id)
#         print(f"Bot: {response}")


